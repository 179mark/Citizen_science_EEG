{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we compare models of different depth and architecture on clear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use(\"TkAgg\")\n",
    "\n",
    "from sklearn.preprocessing import normalize#, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "import mne\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, GRU, LSTM, Lambda, RepeatVector, Reshape, Dropout, Conv1D, UpSampling1D, Bidirectional\n",
    "\n",
    "from os import walk, listdir\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "data_path = \"../../data/train/\"\n",
    "clear_data_path = \"/media/valbub/Docs/data/train/\"\n",
    "raw_data_path = \"../../data/resting_state/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(object):\n",
    "    def __init__(self, \n",
    "             input_dim = (7, 58), \n",
    "             encoded_dim = (1, 58), \n",
    "             loss=\"mse\", \n",
    "             optimizer=\"adadelta\", \n",
    "             activation=(\"relu\", \"sigmoid\", \"tanh\", \"elu\"),\n",
    "             act_idx=(0, 0),\n",
    "             kernel = 7):\n",
    "        \n",
    "            self.input_dim = input_dim\n",
    "            self.encoded_dim = encoded_dim\n",
    "            \n",
    "            class MinMaxScaler():\n",
    "\n",
    "                def __init__(self, minimum=None, maximum=None):\n",
    "                    self.minimum = minimum\n",
    "                    self.maximum = maximum\n",
    "\n",
    "                def fit_transform(self, X):\n",
    "                    if self.minimum is None or self.maximum is None:\n",
    "                        self.minimum = np.min(X, axis=(0, 1))\n",
    "                        self.maximum = np.max(X, axis=(0, 1))\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def transform(self, X):\n",
    "                    return (np.array(X) - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def reverse_transform(self, X_scl):\n",
    "                    return X_scl * (self.maximum - self.minimum) + self.minimum\n",
    "\n",
    "            self.scaler = MinMaxScaler()\n",
    "    \n",
    "    def prepare_clear_data(self, data_path, limit=2):\n",
    "        train_eeg_dir = data_path\n",
    "        train_eeg_names = [x for x in listdir(train_eeg_dir) \n",
    "                         if x[-3:] == \".h5\"]\n",
    "        data = np.zeros((0, self.input_dim[0], self.input_dim[1]))\n",
    "\n",
    "        flag = 0\n",
    "        err_files = 0\n",
    "        for eeg_name in train_eeg_names:\n",
    "            if flag == limit:\n",
    "                break\n",
    "            flag += 1\n",
    "            h5_file = h5py.File(train_eeg_dir + eeg_name, 'r')\n",
    "            a_group_key = list(h5_file.keys())[0]\n",
    "            eeg_data = np.array(h5_file[a_group_key]).T\n",
    "            batches = np.array(self._getBatches(eeg_data, batch_size=self.input_dim[0]))\n",
    "            data = np.concatenate((data, batches), axis=0)\n",
    "        return data\n",
    "    \n",
    "    def prepare_raw_data(self, data_path, limit=2):\n",
    "        def preparefile(file_path):\n",
    "            raw = np.array(mne.io.read_raw_brainvision(file_path, preload=True).to_data_frame())\n",
    "            batches = np.array(self._getBatches(raw, batch_size=self.input_dim[0]))\n",
    "            del raw\n",
    "            return batches\n",
    "        files = []\n",
    "        data = []\n",
    "        for elem in walk(data_path):\n",
    "            for file in elem[-1]:\n",
    "                if file[-4:] == \"vhdr\":\n",
    "                    files.append(file)\n",
    "        data = np.ndarray(shape=(0, self.input_dim[0], self.input_dim[1]))\n",
    "        flag = 0\n",
    "        for file in files:\n",
    "            flag += 1\n",
    "            file_name = data_path + file\n",
    "            if flag == limit:\n",
    "                break\n",
    "            batches =  preparefile(file_name)\n",
    "            data = np.concatenate((data, batches), axis=0)\n",
    "        return data\n",
    "    \n",
    "    def fit(self, X_train, epochs=50):\n",
    "        X_scaled = self.scaler.fit_transform(X_train)\n",
    "        return self.autoencoder.fit(X_scaled, X_scaled, epochs = epochs)\n",
    "    \n",
    "    def fit_scaler(self, X_train):\n",
    "        self.scaler.fit_transform(X_train)\n",
    "    \n",
    "    def encode(self, df):\n",
    "        return self._predict(df, self.encoder, self.input_dim[0])\n",
    "    \n",
    "    def decode(self, df):\n",
    "        return self._predict(df, self.decoder, self.encoded_dim[1])\n",
    "    \n",
    "    def run(self, df):\n",
    "        return self._predict(df, self.autoencoder, self.input_dim[0])\n",
    "    \n",
    "    def save(self, path, part=\"autoencoder\"):\n",
    "        if part == \"encoder\":\n",
    "            self.encoder.save(path)\n",
    "        elif part == \"decoder\":\n",
    "            self.decoder.save(path)\n",
    "        elif part == \"autoencoder\":\n",
    "            self.autoencoder.save(path)\n",
    "        pass\n",
    "    \n",
    "    def load(self, path, part=\"autoencoder\", X_train=None):\n",
    "        if part == \"encoder\":\n",
    "            self.encoder = keras.models.load_model(path)\n",
    "        elif part == \"decoder\":\n",
    "            self.decoder = keras.models.load_model(path)\n",
    "        elif part == \"autoencoder\":\n",
    "            self.autoencoder = keras.models.load_model(path)\n",
    "        if x_train is not None:\n",
    "            self.fit_scaler(X_train)\n",
    "    \n",
    "\n",
    "    def _predict(self, df, model):\n",
    "        batches = self.scaler.transform(df)\n",
    "        batches = tuple(self._predictBatch(batch.reshape((1, *batch.shape)), model) for batch in batches)\n",
    "        batches = self._concatBatches(batches) \n",
    "        return self.scaler.reverse_transform(batches)\n",
    "    \n",
    "    def _predictBatch(self, batch, model):\n",
    "        return model.predict(batch)\n",
    "    \n",
    "    def _getBatches(self, arr, batch_size, axis=0):\n",
    "        n_batches = arr.shape[axis] // batch_size\n",
    "        result = np.array_split(arr, n_batches, axis=axis)\n",
    "        i = 0\n",
    "        while result[i].shape[0] != batch_size:\n",
    "            i += 1\n",
    "        result = result[i:]\n",
    "        return result\n",
    "    \n",
    "    def _concatBatches(self, batches, axis=0):\n",
    "        return np.concatenate(batches, axis=axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AU_Stage_1(AutoEncoder):\n",
    "        def __init__(self, \n",
    "                 input_dim = (7, 58), \n",
    "                 encoded_dim = (1, 58), \n",
    "                 loss=\"mse\", \n",
    "                 optimizer=\"adadelta\", \n",
    "                 activation=(\"elu\", \"sigmoid\"),\n",
    "                 kernel = 7):\n",
    "        \n",
    "            self.input_dim = input_dim\n",
    "            self.encoded_dim = encoded_dim\n",
    "\n",
    "            #Encoder\n",
    "            self._inputs = Input(shape=input_dim)\n",
    "            self._flat = Flatten()(self._inputs)\n",
    "            self._dense = Dense(units=np.prod(encoded_dim), activation=activation[0])(self._flat)\n",
    "            self._encoded = Reshape(encoded_dim)(self._dense)\n",
    "\n",
    "            #Decoder\n",
    "            self._encoded_inputs = Input(shape=(encoded_dim[0]*encoded_dim[1],))\n",
    "            self._flat_decoded = Dense(input_dim[0]*input_dim[1], activation=activation[1])(self._encoded_inputs)\n",
    "            self._decoded = Reshape(input_dim)(self._flat_decoded)\n",
    "\n",
    "            #Models\n",
    "            self.encoder = Model(self._inputs, self._encoded)\n",
    "            self.decoder = Model(self._encoded_inputs, self._decoded)\n",
    "            self.autoencoder = Model(self._inputs, self.decoder(self.encoder(self._inputs)))\n",
    "            \n",
    "            self.autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "            class MinMaxScaler():\n",
    "\n",
    "                def __init__(self, minimum=None, maximum=None):\n",
    "                    self.minimum = minimum\n",
    "                    self.maximum = maximum\n",
    "\n",
    "                def fit_transform(self, X):\n",
    "                    if self.minimum is None or self.maximum is None:\n",
    "                        self.minimum = np.min(X, axis=(0, 1))\n",
    "                        self.maximum = np.max(X, axis=(0, 1))\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def transform(self, X):\n",
    "                    return (np.array(X) - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def reverse_transform(self, X_scl):\n",
    "                    return X_scl * (self.maximum - self.minimum) + self.minimum\n",
    "\n",
    "            self.scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AU_Stage_2(AutoEncoder):\n",
    "        def __init__(self, \n",
    "                 input_dim = (7, 58), \n",
    "                 encoded_dim = (1, 58), \n",
    "                 loss=\"mse\", \n",
    "                 optimizer=\"adadelta\", \n",
    "                 activation=(\"elu\", \"sigmoid\"),\n",
    "                 kernel = 7):\n",
    "        \n",
    "            self.input_dim = input_dim\n",
    "            self.encoded_dim = encoded_dim\n",
    "\n",
    "            #Encoder\n",
    "            self._inputs = Input(shape=input_dim)\n",
    "            self._conv = Conv1D(filters=encoded_dim[1], kernel_size=kernel)(self._inputs) \n",
    "            self._dense = Dense(units=np.prod(encoded_dim), activation=activation[0])(self._conv)\n",
    "            self._encoded = Reshape(encoded_dim)(self._dense)\n",
    "\n",
    "            #Decoder\n",
    "            self._encoded_inputs = Input(shape=encoded_dim)\n",
    "            self._flat_decoded = Dense(units=np.prod(input_dim), activation=activation[1])(self._encoded_inputs)\n",
    "            self._decoded = Reshape(input_dim)(self._flat_decoded)\n",
    "\n",
    "            #Models\n",
    "            self.encoder = Model(self._inputs, self._encoded)\n",
    "            self.decoder = Model(self._encoded_inputs, self._decoded)\n",
    "            self.autoencoder = Model(self._inputs, self.decoder(self.encoder(self._inputs)))\n",
    "\n",
    "            self.autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "            \n",
    "            class MinMaxScaler():\n",
    "\n",
    "                def __init__(self, minimum=None, maximum=None):\n",
    "                    self.minimum = minimum\n",
    "                    self.maximum = maximum\n",
    "\n",
    "                def fit_transform(self, X):\n",
    "                    if self.minimum is None or self.maximum is None:\n",
    "                        self.minimum = np.min(X, axis=(0, 1))\n",
    "                        self.maximum = np.max(X, axis=(0, 1))\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def transform(self, X):\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def reverse_transform(self, X_scl):\n",
    "                    return X_scl * (self.maximum - self.minimum) + self.minimum\n",
    "\n",
    "            self.scaler = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AU_Stage_3(AutoEncoder):\n",
    "        def __init__(self, \n",
    "                 input_dim = (7, 58), \n",
    "                 encoded_dim = (1, 58), \n",
    "                 loss=\"mse\", \n",
    "                 optimizer=\"adadelta\", \n",
    "                 activation=(\"elu\", \"sigmoid\"),\n",
    "                 kernel = 7,\n",
    "                 folds=100):\n",
    "        \n",
    "            self.input_dim = input_dim\n",
    "            self.encoded_dim = encoded_dim\n",
    "\n",
    "            #Encoder\n",
    "            self._inputs = Input(shape=input_dim)\n",
    "            self._lambda = Lambda(lambda x: K.round(x * folds) / folds)(self._inputs)\n",
    "            self._lstm = LSTM(input_dim[1], return_sequences=True, dropout=0, recurrent_dropout=0.1)(self._lambda)\n",
    "            self._conv = Conv1D(filters=encoded_dim[1], kernel_size=input_dim[0])(self._lstm)\n",
    "            self._dense = Dense(units=np.prod(encoded_dim), activation=activation[0])(self._conv)\n",
    "            self._encoded = Reshape(encoded_dim)(self._dense)\n",
    "            \n",
    "            #Decoder\n",
    "            self._encoded_inputs = Input(shape=encoded_dim)\n",
    "            self._flat_decoded_1 = Dense(units=np.prod(input_dim), activation=activation[1])(self._encoded_inputs)\n",
    "            self._flat_decoded_2 = UpSampling1D(size=input_dim[0] // encoded_dim[0])(self._flat_decoded_1)\n",
    "            self._lstm_2 = LSTM(input_dim[1], return_sequences=True)(self._flat_decoded_2)\n",
    "            self._decoded = Reshape(input_dim)(self._lstm_2)\n",
    "            \n",
    "            #Models\n",
    "            self.encoder = Model(self._inputs, self._encoded)\n",
    "            self.decoder = Model(self._encoded_inputs, self._decoded)\n",
    "            self.autoencoder = Model(self._inputs, self.decoder(self.encoder(self._inputs)))\n",
    "            \n",
    "            self.autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "            \n",
    "            class MinMaxScaler():\n",
    "\n",
    "                def __init__(self, minimum=None, maximum=None):\n",
    "                    self.minimum = minimum\n",
    "                    self.maximum = maximum\n",
    "\n",
    "                def fit_transform(self, X):\n",
    "                    if self.minimum is None or self.maximum is None:\n",
    "                        self.minimum = np.min(X, axis=(0, 1))\n",
    "                        self.maximum = np.max(X, axis=(0, 1))  + 1e-10\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def transform(self, X):\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def reverse_transform(self, X_scl):\n",
    "                    return X_scl * (self.maximum - self.minimum) + self.minimum\n",
    "\n",
    "            self.scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AU_Stage_3_5(AutoEncoder):\n",
    "        def __init__(self, \n",
    "                 input_dim = (7, 58), \n",
    "                 encoded_dim = (1, 58), \n",
    "                 loss=\"mse\", \n",
    "                 optimizer=\"adadelta\", \n",
    "                 activation=(\"elu\", \"sigmoid\"),\n",
    "                 kernel = 7):\n",
    "        \n",
    "            self.input_dim = input_dim\n",
    "            self.encoded_dim = encoded_dim\n",
    "\n",
    "            #Encoder\n",
    "            self._inputs = Input(shape=input_dim)\n",
    "            self._gru = GRU(input_dim[1], return_sequences=True, dropout=0, recurrent_dropout=0.1)(self._inputs)\n",
    "            self._conv = Conv1D(filters=encoded_dim[1], kernel_size=input_dim[0])(self._lstm)\n",
    "            self._dense = Dense(units=np.prod(encoded_dim), activation=activation[0])(self._conv)\n",
    "            self._encoded = Reshape(encoded_dim)(self._dense)\n",
    "\n",
    "            #Decoder\n",
    "            self._encoded_inputs = Input(shape=encoded_dim)\n",
    "            self._flat_decoded_1 = Dense(units=np.prod(input_dim), activation=activation[1])(self._encoded_inputs)\n",
    "            self._flat_decoded_2 = UpSampling1D(size=input_dim[0] // encoded_dim[0])(self._flat_decoded_1)\n",
    "            self._gru_2 = GRU(input_dim[1], return_sequences=True)(self._flat_decoded_2)\n",
    "            self._decoded = Reshape(input_dim)(self._lstm_2)\n",
    "\n",
    "            #Models\n",
    "            self.encoder = Model(self._inputs, self._encoded)\n",
    "            self.decoder = Model(self._encoded_inputs, self._decoded)\n",
    "            self.autoencoder = Model(self._inputs, self.decoder(self.encoder(self._inputs)))\n",
    "\n",
    "            self.autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "            class MinMaxScaler():\n",
    "\n",
    "                def __init__(self, minimum=None, maximum=None):\n",
    "                    self.minimum = minimum\n",
    "                    self.maximum = maximum\n",
    "\n",
    "                def fit_transform(self, X):\n",
    "                    if self.minimum is None or self.maximum is None:\n",
    "                        self.minimum = np.min(X, axis=(0, 1))\n",
    "                        self.maximum = np.max(X, axis=(0, 1)) + 1e-10\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def transform(self, X):\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def reverse_transform(self, X_scl):\n",
    "                    return X_scl * (self.maximum - self.minimum) + self.minimum\n",
    "\n",
    "            self.scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AU_Stage_4(AutoEncoder):\n",
    "        def __init__(self, \n",
    "                 input_dim = (7, 58), \n",
    "                 encoded_dim = (1, 58), \n",
    "                 loss=\"mse\", \n",
    "                 optimizer=\"adadelta\", \n",
    "                 activation=(\"elu\", \"sigmoid\"),\n",
    "                 kernel = 7,\n",
    "                 folds = 100):\n",
    "        \n",
    "            self.input_dim = input_dim\n",
    "            self.encoded_dim = encoded_dim\n",
    "\n",
    "            #Encoder\n",
    "            self._inputs = Input(shape=input_dim)\n",
    "            self._lambda = Lambda(lambda x: K.round(x * folds) / folds)(self._inputs)\n",
    "            self._lstm = LSTM(input_dim[1], return_sequences=True, dropout=0, recurrent_dropout=0.1)(self._lambda)\n",
    "            self._reshape = Reshape((-1, input_dim[1] * input_dim[0]))(self._lstm)\n",
    "            self._dense = Dense(units=np.prod(encoded_dim), activation=activation[0])(self._reshape)\n",
    "            self._encoded = Reshape(encoded_dim)(self._dense)\n",
    "\n",
    "            #Decoder\n",
    "            self._encoded_inputs = Input(shape=encoded_dim)\n",
    "            self._flat_decoded_1 = Dense(units=np.prod(input_dim), activation=activation[1])(self._encoded_inputs)\n",
    "            self._lstm_2 = LSTM(input_dim[1] * input_dim[0], return_sequences=True)(self._flat_decoded_1)\n",
    "            self._decoded = Reshape((-1, input_dim[1]))(self._lstm_2)\n",
    "\n",
    "            #Models\n",
    "            self.encoder = Model(self._inputs, self._encoded)\n",
    "            self.decoder = Model(self._encoded_inputs, self._decoded)\n",
    "            self.autoencoder = Model(self._inputs, self.decoder(self.encoder(self._inputs)))\n",
    "\n",
    "            self.autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "            class MinMaxScaler():\n",
    "\n",
    "                def __init__(self, minimum=None, maximum=None):\n",
    "                    self.minimum = minimum\n",
    "                    self.maximum = maximum\n",
    "\n",
    "                def fit_transform(self, X):\n",
    "                    if self.minimum is None or self.maximum is None:\n",
    "                        self.minimum = np.min(X, axis=(0, 1))\n",
    "                        self.maximum = np.max(X, axis=(0, 1)) + 1e-10\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def transform(self, X):\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def reverse_transform(self, X_scl):\n",
    "                    return X_scl * (self.maximum - self.minimum) + self.minimum\n",
    "\n",
    "            self.scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epoch_numb = 50\n",
    "limit = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1188300, 7, 58)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we get data for all experiments in future\n",
    "au_1 = AU_Stage_1()\n",
    "data_set = au_1.prepare_clear_data(clear_data_path, limit=limit)\n",
    "train_data, test_data = train_test_split(data_set, random_state=0, test_size=0.3)\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 4.8550e-04\n",
      "Epoch 2/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 2.9308e-04\n",
      "Epoch 3/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 1.7581e-04\n",
      "Epoch 4/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 1.4471e-04\n",
      "Epoch 5/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 1.2895e-04\n",
      "Epoch 6/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 1.1818e-04\n",
      "Epoch 7/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 1.1131e-04\n",
      "Epoch 8/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 1.0636e-04\n",
      "Epoch 9/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 1.0206e-04\n",
      "Epoch 10/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 9.7956e-05\n",
      "Epoch 11/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 9.4017e-05\n",
      "Epoch 12/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 9.0315e-05\n",
      "Epoch 13/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 8.6910e-05\n",
      "Epoch 14/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 8.3833e-05\n",
      "Epoch 15/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 8.1074e-05\n",
      "Epoch 16/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 7.8607e-05\n",
      "Epoch 17/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 7.6387e-05\n",
      "Epoch 18/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 7.4368e-05\n",
      "Epoch 19/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 7.2509e-05\n",
      "Epoch 20/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 7.0782e-05\n",
      "Epoch 21/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 6.9158e-05\n",
      "Epoch 22/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 6.7625e-05\n",
      "Epoch 23/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 6.6170e-05\n",
      "Epoch 24/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 6.4792e-05\n",
      "Epoch 25/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 6.3488e-05\n",
      "Epoch 26/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 6.2256e-05\n",
      "Epoch 27/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 6.1094e-05\n",
      "Epoch 28/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.9996e-05\n",
      "Epoch 29/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.8960e-05\n",
      "Epoch 30/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.7987e-05\n",
      "Epoch 31/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.7069e-05\n",
      "Epoch 32/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.6206e-05\n",
      "Epoch 33/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.5392e-05\n",
      "Epoch 34/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.4627e-05\n",
      "Epoch 35/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.3902e-05\n",
      "Epoch 36/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.3217e-05\n",
      "Epoch 37/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.2567e-05\n",
      "Epoch 38/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.1946e-05\n",
      "Epoch 39/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.1353e-05\n",
      "Epoch 40/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.0785e-05\n",
      "Epoch 41/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 5.0238e-05\n",
      "Epoch 42/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 4.9711e-05\n",
      "Epoch 43/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 4.9200e-05\n",
      "Epoch 44/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 4.8705e-05\n",
      "Epoch 45/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 4.8222e-05\n",
      "Epoch 46/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 4.7753e-05\n",
      "Epoch 47/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 4.7294e-05\n",
      "Epoch 48/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 4.6846e-05\n",
      "Epoch 49/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 4.6410e-05\n",
      "Epoch 50/50\n",
      "831810/831810 [==============================] - 39s 47us/step - loss: 4.5980e-05\n"
     ]
    }
   ],
   "source": [
    "au_1.fit(train_data, epochs=epoch_numb)\n",
    "au_1.save(\"./model_stage_1/au\")\n",
    "au_1.save(\"./model_stage_1/en\")\n",
    "au_1.save(\"./model_stage_1/de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dense\n",
      "\n",
      "Clear data\n",
      "0.8247301200656326\n",
      "4.832945938199143e-06\n",
      "7.168427148160001e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr = au_1._predict(test_data, au_1.autoencoder)\n",
    "ds = np.concatenate(test_data)\n",
    "p = np.concatenate(pr)\n",
    "print('\\nDense')\n",
    "print('\\nClear data')\n",
    "print(r2_score(ds, p))\n",
    "print(mean_absolute_error(ds, p))\n",
    "print(mean_squared_error(ds, p))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del au_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 4.8277e-04\n",
      "Epoch 2/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 2.1755e-04\n",
      "Epoch 3/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 1.5477e-04\n",
      "Epoch 4/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 1.3684e-04\n",
      "Epoch 5/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 1.2202e-04\n",
      "Epoch 6/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 1.1387e-04\n",
      "Epoch 7/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 1.0868e-04\n",
      "Epoch 8/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 1.0385e-04\n",
      "Epoch 9/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 9.8836e-05\n",
      "Epoch 10/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 9.3909e-05\n",
      "Epoch 11/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 8.9421e-05\n",
      "Epoch 12/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 8.5510e-05\n",
      "Epoch 13/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 8.2172e-05\n",
      "Epoch 14/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 7.9332e-05\n",
      "Epoch 15/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 7.6853e-05\n",
      "Epoch 16/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 7.4622e-05\n",
      "Epoch 17/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 7.2554e-05\n",
      "Epoch 18/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 7.0607e-05\n",
      "Epoch 19/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 6.8774e-05\n",
      "Epoch 20/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 6.7046e-05\n",
      "Epoch 21/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 6.5427e-05\n",
      "Epoch 22/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 6.3924e-05\n",
      "Epoch 23/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 6.2524e-05\n",
      "Epoch 24/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 6.1234e-05\n",
      "Epoch 25/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 6.0041e-05\n",
      "Epoch 26/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 5.8940e-05\n",
      "Epoch 27/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 5.7929e-05\n",
      "Epoch 28/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 5.6984e-05\n",
      "Epoch 29/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 5.6120e-05\n",
      "Epoch 30/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 5.5306e-05\n",
      "Epoch 31/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 5.4547e-05\n",
      "Epoch 32/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 5.3831e-05\n",
      "Epoch 33/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 5.3153e-05\n",
      "Epoch 34/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 5.2505e-05\n",
      "Epoch 35/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 5.1891e-05\n",
      "Epoch 36/50\n",
      "831810/831810 [==============================] - 41s 49us/step - loss: 5.1298e-05\n",
      "Epoch 37/50\n",
      "831810/831810 [==============================] - 40s 49us/step - loss: 5.0731e-05\n",
      "Epoch 38/50\n",
      "831810/831810 [==============================] - 40s 48us/step - loss: 5.0180e-05\n",
      "Epoch 39/50\n",
      "831810/831810 [==============================] - 40s 48us/step - loss: 4.9645e-05\n",
      "Epoch 40/50\n",
      "831810/831810 [==============================] - 40s 48us/step - loss: 4.9123e-05\n",
      "Epoch 41/50\n",
      "831810/831810 [==============================] - 40s 48us/step - loss: 4.8614e-05\n",
      "Epoch 42/50\n",
      "831810/831810 [==============================] - 40s 48us/step - loss: 4.8113e-05\n",
      "Epoch 43/50\n",
      "831810/831810 [==============================] - 40s 48us/step - loss: 4.7632e-05\n",
      "Epoch 44/50\n",
      "831810/831810 [==============================] - 40s 48us/step - loss: 4.7146e-05\n",
      "Epoch 45/50\n",
      "831810/831810 [==============================] - 40s 48us/step - loss: 4.6677e-05\n",
      "Epoch 46/50\n",
      "831810/831810 [==============================] - 40s 49us/step - loss: 4.6203e-05\n",
      "Epoch 47/50\n",
      "831810/831810 [==============================] - 40s 48us/step - loss: 4.5750e-05\n",
      "Epoch 48/50\n",
      "831810/831810 [==============================] - 40s 48us/step - loss: 4.5285e-05\n",
      "Epoch 49/50\n",
      "831810/831810 [==============================] - 40s 48us/step - loss: 4.4852e-05\n",
      "Epoch 50/50\n",
      "831810/831810 [==============================] - 40s 48us/step - loss: 4.4413e-05\n"
     ]
    }
   ],
   "source": [
    "au_2 = AU_Stage_2()\n",
    "# here we assume that batches are the same and use data from previous model to compare models correctly\n",
    "# data_set = au_2.prepare_clear_data(clear_data_path, limit=limit)\n",
    "# train_data, test_data = train_test_split(data_set, random_state=0, test_size=0.3)\n",
    "\n",
    "au_2.fit(train_data, epochs=epoch_numb)\n",
    "au_2.save(\"./model_stage_2/au\")\n",
    "au_2.save(\"./model_stage_2/en\")\n",
    "au_2.save(\"./model_stage_2/de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dense+Conv\n",
      "\n",
      "Clear data\n",
      "0.8257848227367229\n",
      "5.193955342016033e-06\n",
      "8.964024932534042e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr = au_2._predict(test_data, au_2.autoencoder)\n",
    "ds = np.concatenate(test_data)\n",
    "p = np.concatenate(pr)\n",
    "print('\\nDense+Conv')\n",
    "print('\\nClear data')\n",
    "print(r2_score(ds, p))\n",
    "print(mean_absolute_error(ds, p))\n",
    "print(mean_squared_error(ds, p))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del au_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "831810/831810 [==============================] - 195s 234us/step - loss: 6.1618e-04\n",
      "Epoch 2/50\n",
      "831810/831810 [==============================] - 194s 234us/step - loss: 3.1320e-04\n",
      "Epoch 3/50\n",
      "831810/831810 [==============================] - 194s 234us/step - loss: 1.6809e-04\n",
      "Epoch 4/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 1.3930e-04\n",
      "Epoch 5/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 1.3372e-04\n",
      "Epoch 6/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 1.3095e-04\n",
      "Epoch 7/50\n",
      "831810/831810 [==============================] - 194s 234us/step - loss: 1.2689e-04\n",
      "Epoch 8/50\n",
      "831810/831810 [==============================] - 194s 234us/step - loss: 1.1984e-04\n",
      "Epoch 9/50\n",
      "831810/831810 [==============================] - 194s 234us/step - loss: 1.1493e-04\n",
      "Epoch 10/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 1.1277e-04\n",
      "Epoch 11/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 1.1157e-04\n",
      "Epoch 12/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 1.1059e-04\n",
      "Epoch 13/50\n",
      "831810/831810 [==============================] - 194s 234us/step - loss: 1.0957e-04\n",
      "Epoch 14/50\n",
      "831810/831810 [==============================] - 194s 234us/step - loss: 1.0828e-04\n",
      "Epoch 15/50\n",
      "831810/831810 [==============================] - 194s 234us/step - loss: 1.0654e-04\n",
      "Epoch 16/50\n",
      "831810/831810 [==============================] - 194s 234us/step - loss: 1.0403e-04\n",
      "Epoch 17/50\n",
      "831810/831810 [==============================] - 194s 234us/step - loss: 1.0100e-04\n",
      "Epoch 18/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 9.8006e-05\n",
      "Epoch 19/50\n",
      "831810/831810 [==============================] - 194s 234us/step - loss: 9.5957e-05\n",
      "Epoch 20/50\n",
      "831810/831810 [==============================] - 194s 234us/step - loss: 9.4636e-05\n",
      "Epoch 21/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 9.3596e-05\n",
      "Epoch 22/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 9.2633e-05\n",
      "Epoch 23/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 9.1486e-05\n",
      "Epoch 24/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.9967e-05\n",
      "Epoch 25/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.8366e-05\n",
      "Epoch 26/50\n",
      "831810/831810 [==============================] - 194s 234us/step - loss: 8.7077e-05\n",
      "Epoch 27/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.6161e-05\n",
      "Epoch 28/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.5572e-05\n",
      "Epoch 29/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.5087e-05\n",
      "Epoch 30/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.4636e-05\n",
      "Epoch 31/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.4213e-05\n",
      "Epoch 32/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.3705e-05\n",
      "Epoch 33/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.3191e-05\n",
      "Epoch 34/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.2597e-05\n",
      "Epoch 35/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.1989e-05\n",
      "Epoch 36/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.1484e-05\n",
      "Epoch 37/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.0886e-05\n",
      "Epoch 38/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 8.0407e-05\n",
      "Epoch 39/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 7.9918e-05\n",
      "Epoch 40/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 7.9373e-05\n",
      "Epoch 41/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 7.8911e-05\n",
      "Epoch 42/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 7.8369e-05\n",
      "Epoch 43/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 7.7879e-05\n",
      "Epoch 44/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 7.7371e-05\n",
      "Epoch 45/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 7.6862e-05\n",
      "Epoch 46/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 7.6338e-05\n",
      "Epoch 47/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 7.5853e-05\n",
      "Epoch 48/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 7.5367e-05\n",
      "Epoch 49/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 7.4875e-05\n",
      "Epoch 50/50\n",
      "831810/831810 [==============================] - 194s 233us/step - loss: 7.4464e-05\n"
     ]
    }
   ],
   "source": [
    "au_3 = AU_Stage_3()\n",
    "# here we assume that batches are the same and use data from previous model to compare models correctly\n",
    "# data_set = au_3.prepare_clear_data(clear_data_path, limit=limit)\n",
    "# train_data, test_data = train_test_split(data_set, random_state=0, test_size=0.3)\n",
    "\n",
    "au_3.fit(train_data, epochs=epoch_numb)\n",
    "au_3.save(\"./model_stage_3/au\")\n",
    "au_3.save(\"./model_stage_3/en\")\n",
    "au_3.save(\"./model_stage_3/de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dense+Conv+LSTM\n",
      "\n",
      "Clear data\n",
      "0.708720138156052\n",
      "6.462648886844191e-06\n",
      "1.0027366676769321e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr = au_3._predict(test_data, au_3.autoencoder)\n",
    "ds = np.concatenate(test_data)\n",
    "p = np.concatenate(pr)\n",
    "print(\"\\nDense+Conv+LSTM\")\n",
    "print('\\nClear data')\n",
    "print(r2_score(ds, p))\n",
    "print(mean_absolute_error(ds, p))\n",
    "print(mean_squared_error(ds, p))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del au_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "831810/831810 [==============================] - 172s 206us/step - loss: 5.2551e-04\n",
      "Epoch 2/50\n",
      "831810/831810 [==============================] - 171s 205us/step - loss: 1.6351e-04\n",
      "Epoch 3/50\n",
      "831810/831810 [==============================] - 171s 205us/step - loss: 1.3073e-04\n",
      "Epoch 4/50\n",
      "831810/831810 [==============================] - 171s 205us/step - loss: 1.2008e-04\n",
      "Epoch 5/50\n",
      "831810/831810 [==============================] - 171s 205us/step - loss: 1.0964e-04\n",
      "Epoch 6/50\n",
      "831810/831810 [==============================] - 171s 205us/step - loss: 1.0287e-04\n",
      "Epoch 7/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 9.8208e-05\n",
      "Epoch 8/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 9.2654e-05\n",
      "Epoch 9/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 8.7579e-05\n",
      "Epoch 10/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 8.4184e-05\n",
      "Epoch 11/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 8.1254e-05\n",
      "Epoch 12/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 7.9091e-05\n",
      "Epoch 13/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 7.7122e-05\n",
      "Epoch 14/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 7.5595e-05\n",
      "Epoch 15/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 7.4069e-05\n",
      "Epoch 16/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 7.2493e-05\n",
      "Epoch 17/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 7.1262e-05\n",
      "Epoch 18/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 7.0242e-05\n",
      "Epoch 19/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.9160e-05\n",
      "Epoch 20/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.8269e-05\n",
      "Epoch 21/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.7347e-05\n",
      "Epoch 22/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.6585e-05\n",
      "Epoch 23/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.5861e-05\n",
      "Epoch 24/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.5113e-05\n",
      "Epoch 25/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.4405e-05\n",
      "Epoch 26/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.3786e-05\n",
      "Epoch 27/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.3155e-05\n",
      "Epoch 28/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.2538e-05\n",
      "Epoch 29/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.1948e-05\n",
      "Epoch 30/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.1317e-05\n",
      "Epoch 31/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.0709e-05\n",
      "Epoch 32/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 6.0245e-05\n",
      "Epoch 33/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.9779e-05\n",
      "Epoch 34/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.9261e-05\n",
      "Epoch 35/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.8820e-05\n",
      "Epoch 36/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.8308e-05\n",
      "Epoch 37/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.7920e-05\n",
      "Epoch 38/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.7467e-05\n",
      "Epoch 39/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.7057e-05\n",
      "Epoch 40/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.6575e-05\n",
      "Epoch 41/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.6226e-05\n",
      "Epoch 42/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.5797e-05\n",
      "Epoch 43/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.5483e-05\n",
      "Epoch 44/50\n",
      "831810/831810 [==============================] - 170s 204us/step - loss: 5.5047e-05\n",
      "Epoch 45/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.4659e-05\n",
      "Epoch 46/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.4383e-05\n",
      "Epoch 47/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.3979e-05\n",
      "Epoch 48/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.3700e-05\n",
      "Epoch 49/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.3322e-05\n",
      "Epoch 50/50\n",
      "831810/831810 [==============================] - 170s 205us/step - loss: 5.3069e-05\n"
     ]
    }
   ],
   "source": [
    "au_3_5 = AU_Stage_3_5()\n",
    "# here we assume that batches are the same and use data from previous model to compare models correctly\n",
    "# data_set = au_3_5.prepare_clear_data(clear_data_path, limit=limit)\n",
    "# train_data, test_data = train_test_split(data_set, random_state=0, test_size=0.3)\n",
    "\n",
    "au_3_5.fit(train_data, epochs=epoch_numb)\n",
    "au_3_5.save(\"./model_stage_3_5/au\")\n",
    "au_3_5.save(\"./model_stage_3_5/en\")\n",
    "au_3_5.save(\"./model_stage_3_5/de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dense+Conv+GRU\n",
      "\n",
      "Clear data\n",
      "0.7829797791169523\n",
      "6.931606360453831e-06\n",
      "2.0235480838020267e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr = au_3_5._predict(test_data, au_3_5.autoencoder)\n",
    "ds = np.concatenate(test_data)\n",
    "p = np.concatenate(pr)\n",
    "print(\"\\nDense+Conv+GRU\")\n",
    "print('\\nClear data')\n",
    "print(r2_score(ds, p))\n",
    "print(mean_absolute_error(ds, p))\n",
    "print(mean_squared_error(ds, p))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del au_3_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "831810/831810 [==============================] - 516s 621us/step - loss: 7.5567e-04\n",
      "Epoch 2/50\n",
      "831810/831810 [==============================] - 499s 600us/step - loss: 3.0272e-04\n",
      "Epoch 3/50\n",
      "831810/831810 [==============================] - 499s 600us/step - loss: 2.7616e-04\n",
      "Epoch 4/50\n",
      "831810/831810 [==============================] - 500s 601us/step - loss: 2.6703e-04\n",
      "Epoch 5/50\n",
      "831810/831810 [==============================] - 498s 599us/step - loss: 2.6217e-04\n",
      "Epoch 6/50\n",
      "831810/831810 [==============================] - 499s 600us/step - loss: 2.4990e-04\n",
      "Epoch 7/50\n",
      "831810/831810 [==============================] - 499s 600us/step - loss: 1.8682e-04\n",
      "Epoch 8/50\n",
      "831810/831810 [==============================] - 498s 599us/step - loss: 1.6793e-04\n",
      "Epoch 9/50\n",
      "831810/831810 [==============================] - 501s 602us/step - loss: 1.6304e-04\n",
      "Epoch 10/50\n",
      "831810/831810 [==============================] - 501s 602us/step - loss: 1.5942e-04\n",
      "Epoch 11/50\n",
      "831810/831810 [==============================] - 499s 600us/step - loss: 1.5220e-04\n",
      "Epoch 12/50\n",
      "831810/831810 [==============================] - 510s 613us/step - loss: 1.3478e-04\n",
      "Epoch 13/50\n",
      "831810/831810 [==============================] - 501s 602us/step - loss: 1.2347e-04\n",
      "Epoch 14/50\n",
      "831810/831810 [==============================] - 496s 596us/step - loss: 1.1945e-04\n",
      "Epoch 15/50\n",
      "831810/831810 [==============================] - 497s 597us/step - loss: 1.1719e-04\n",
      "Epoch 16/50\n",
      "831810/831810 [==============================] - 497s 597us/step - loss: 1.1489e-04\n",
      "Epoch 17/50\n",
      "831810/831810 [==============================] - 496s 597us/step - loss: 1.1134e-04\n",
      "Epoch 18/50\n",
      "831810/831810 [==============================] - 496s 596us/step - loss: 1.0712e-04\n",
      "Epoch 19/50\n",
      "831810/831810 [==============================] - 496s 596us/step - loss: 1.0425e-04\n",
      "Epoch 20/50\n",
      "831810/831810 [==============================] - 496s 596us/step - loss: 1.0230e-04\n",
      "Epoch 21/50\n",
      "831810/831810 [==============================] - 496s 596us/step - loss: 1.0090e-04\n",
      "Epoch 22/50\n",
      "831810/831810 [==============================] - 495s 595us/step - loss: 9.9825e-05\n",
      "Epoch 23/50\n",
      "831810/831810 [==============================] - 495s 595us/step - loss: 9.8953e-05\n",
      "Epoch 24/50\n",
      "831810/831810 [==============================] - 494s 594us/step - loss: 9.8210e-05\n",
      "Epoch 25/50\n",
      "831810/831810 [==============================] - 494s 594us/step - loss: 9.7532e-05\n",
      "Epoch 26/50\n",
      "831810/831810 [==============================] - 494s 593us/step - loss: 9.6893e-05\n",
      "Epoch 27/50\n",
      "831810/831810 [==============================] - 494s 594us/step - loss: 9.6266e-05\n",
      "Epoch 28/50\n",
      "831810/831810 [==============================] - 491s 591us/step - loss: 9.5609e-05\n",
      "Epoch 29/50\n",
      "831810/831810 [==============================] - 493s 593us/step - loss: 9.4918e-05\n",
      "Epoch 30/50\n",
      "831810/831810 [==============================] - 491s 591us/step - loss: 9.4170e-05\n",
      "Epoch 31/50\n",
      "831810/831810 [==============================] - 491s 590us/step - loss: 9.3329e-05\n",
      "Epoch 32/50\n",
      "831810/831810 [==============================] - 491s 590us/step - loss: 9.2392e-05\n",
      "Epoch 33/50\n",
      "831810/831810 [==============================] - 490s 590us/step - loss: 9.1334e-05\n",
      "Epoch 34/50\n",
      "831810/831810 [==============================] - 490s 589us/step - loss: 9.0154e-05\n",
      "Epoch 35/50\n",
      "831810/831810 [==============================] - 487s 586us/step - loss: 8.8857e-05\n",
      "Epoch 36/50\n",
      "831810/831810 [==============================] - 495s 595us/step - loss: 8.7455e-05\n",
      "Epoch 37/50\n",
      "831810/831810 [==============================] - 512s 616us/step - loss: 8.5940e-05\n",
      "Epoch 38/50\n",
      "831810/831810 [==============================] - 504s 606us/step - loss: 8.4343e-05\n",
      "Epoch 39/50\n",
      "831810/831810 [==============================] - 489s 588us/step - loss: 8.2702e-05\n",
      "Epoch 40/50\n",
      "831810/831810 [==============================] - 489s 588us/step - loss: 8.1130e-05\n",
      "Epoch 41/50\n",
      "831810/831810 [==============================] - 489s 588us/step - loss: 7.9714e-05\n",
      "Epoch 42/50\n",
      "831810/831810 [==============================] - 488s 587us/step - loss: 7.8422e-05\n",
      "Epoch 43/50\n",
      "831810/831810 [==============================] - 484s 582us/step - loss: 7.7221e-05\n",
      "Epoch 44/50\n",
      "831810/831810 [==============================] - 488s 586us/step - loss: 7.6058e-05\n",
      "Epoch 45/50\n",
      "831810/831810 [==============================] - 489s 587us/step - loss: 7.4874e-05\n",
      "Epoch 46/50\n",
      "831810/831810 [==============================] - 488s 587us/step - loss: 7.3612e-05\n",
      "Epoch 47/50\n",
      "831810/831810 [==============================] - 488s 587us/step - loss: 7.2208e-05\n",
      "Epoch 48/50\n",
      "831810/831810 [==============================] - 488s 587us/step - loss: 7.0745e-05\n",
      "Epoch 49/50\n",
      "831810/831810 [==============================] - 488s 587us/step - loss: 6.9343e-05\n",
      "Epoch 50/50\n",
      "831810/831810 [==============================] - 488s 587us/step - loss: 6.8044e-05\n"
     ]
    }
   ],
   "source": [
    "au_4 = AU_Stage_4()\n",
    "# here we assume that batches are the same and use data from previous model to compare models correctly\n",
    "# data_set = au_4.prepare_clear_data(clear_data_path, limit=limit)\n",
    "# train_data, test_data = train_test_split(data_set, random_state=0, test_size=0.3)\n",
    "\n",
    "au_4.fit(train_data, epochs=epoch_numb)\n",
    "au_4.save(\"./model_stage_4/au\")\n",
    "au_4.save(\"./model_stage_4/en\")\n",
    "au_4.save(\"./model_stage_4/de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dense+LSTM\n",
      "\n",
      "Clear data\n",
      "0.7670855327489398\n",
      "4.996688295940117e-06\n",
      "1.001025860930805e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr = au_4._predict(test_data, au_4.autoencoder)\n",
    "ds = np.concatenate(test_data)\n",
    "p = np.concatenate(pr)\n",
    "print(\"\\nDense+LSTM\")\n",
    "print('\\nClear data')\n",
    "print(r2_score(ds, p))\n",
    "print(mean_absolute_error(ds, p))\n",
    "print(mean_squared_error(ds, p))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del au_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "    + попробовать обучить всё это же, но без лямбды\n",
    "    + попробовать размер батча 5 (experiment-1_batch5.ipynb)\n",
    "    + попробовать сжимать из (7, 58) в (7, 29) ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
