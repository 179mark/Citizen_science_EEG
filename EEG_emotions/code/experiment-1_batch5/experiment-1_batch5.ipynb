{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we compare models of different depth and architecture on clear data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use(\"TkAgg\")\n",
    "\n",
    "from sklearn.preprocessing import normalize#, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import h5py\n",
    "import mne\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Reshape, GRU, LSTM, Lambda, RepeatVector, Reshape, Dropout, Conv1D, UpSampling1D, Bidirectional\n",
    "\n",
    "from os import walk, listdir\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "data_path = \"../../data/train/\"\n",
    "clear_data_path = \"/media/valbub/Docs/data/train/\"\n",
    "raw_data_path = \"../../data/resting_state/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(object):\n",
    "    def __init__(self, \n",
    "             input_dim = (5, 58), \n",
    "             encoded_dim = (1, 58), \n",
    "             loss=\"mse\", \n",
    "             optimizer=\"adadelta\", \n",
    "             activation=(\"relu\", \"sigmoid\", \"tanh\", \"elu\"),\n",
    "             act_idx=(0, 0),\n",
    "             kernel = 5):\n",
    "        \n",
    "            self.input_dim = input_dim\n",
    "            self.encoded_dim = encoded_dim\n",
    "            \n",
    "            class MinMaxScaler():\n",
    "\n",
    "                def __init__(self, minimum=None, maximum=None):\n",
    "                    self.minimum = minimum\n",
    "                    self.maximum = maximum\n",
    "\n",
    "                def fit_transform(self, X):\n",
    "                    if self.minimum is None or self.maximum is None:\n",
    "                        self.minimum = np.min(X, axis=(0, 1))\n",
    "                        self.maximum = np.max(X, axis=(0, 1))\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def transform(self, X):\n",
    "                    return (np.array(X) - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def reverse_transform(self, X_scl):\n",
    "                    return X_scl * (self.maximum - self.minimum) + self.minimum\n",
    "\n",
    "            self.scaler = MinMaxScaler()\n",
    "    \n",
    "    def prepare_clear_data(self, data_path, limit=2):\n",
    "        train_eeg_dir = data_path\n",
    "        train_eeg_names = [x for x in listdir(train_eeg_dir) \n",
    "                         if x[-3:] == \".h5\"]\n",
    "        data = np.zeros((0, self.input_dim[0], self.input_dim[1]))\n",
    "\n",
    "        flag = 0\n",
    "        err_files = 0\n",
    "        for eeg_name in train_eeg_names:\n",
    "            if flag == limit:\n",
    "                break\n",
    "            flag += 1\n",
    "            h5_file = h5py.File(train_eeg_dir + eeg_name, 'r')\n",
    "            a_group_key = list(h5_file.keys())[0]\n",
    "            eeg_data = np.array(h5_file[a_group_key]).T\n",
    "            batches = np.array(self._getBatches(eeg_data, batch_size=self.input_dim[0]))\n",
    "            data = np.concatenate((data, batches), axis=0)\n",
    "        return data\n",
    "    \n",
    "    def prepare_raw_data(self, data_path, limit=2):\n",
    "        def preparefile(file_path):\n",
    "            raw = np.array(mne.io.read_raw_brainvision(file_path, preload=True).to_data_frame())\n",
    "            batches = np.array(self._getBatches(raw, batch_size=self.input_dim[0]))\n",
    "            del raw\n",
    "            return batches\n",
    "        files = []\n",
    "        data = []\n",
    "        for elem in walk(data_path):\n",
    "            for file in elem[-1]:\n",
    "                if file[-4:] == \"vhdr\":\n",
    "                    files.append(file)\n",
    "        data = np.ndarray(shape=(0, self.input_dim[0], self.input_dim[1]))\n",
    "        flag = 0\n",
    "        for file in files:\n",
    "            flag += 1\n",
    "            file_name = data_path + file\n",
    "            if flag == limit:\n",
    "                break\n",
    "            batches =  preparefile(file_name)\n",
    "            data = np.concatenate((data, batches), axis=0)\n",
    "        return data\n",
    "    \n",
    "    def fit(self, X_train, epochs=50):\n",
    "        X_scaled = self.scaler.fit_transform(X_train)\n",
    "        return self.autoencoder.fit(X_scaled, X_scaled, epochs = epochs)\n",
    "    \n",
    "    def fit_scaler(self, X_train):\n",
    "        self.scaler.fit_transform(X_train)\n",
    "    \n",
    "    def encode(self, df):\n",
    "        return self._predict(df, self.encoder, self.input_dim[0])\n",
    "    \n",
    "    def decode(self, df):\n",
    "        return self._predict(df, self.decoder, self.encoded_dim[1])\n",
    "    \n",
    "    def run(self, df):\n",
    "        return self._predict(df, self.autoencoder, self.input_dim[0])\n",
    "    \n",
    "    def save(self, path, part=\"autoencoder\"):\n",
    "        if part == \"encoder\":\n",
    "            self.encoder.save(path)\n",
    "        elif part == \"decoder\":\n",
    "            self.decoder.save(path)\n",
    "        elif part == \"autoencoder\":\n",
    "            self.autoencoder.save(path)\n",
    "        pass\n",
    "    \n",
    "    def load(self, path, part=\"autoencoder\", X_train=None):\n",
    "        if part == \"encoder\":\n",
    "            self.encoder = keras.models.load_model(path)\n",
    "        elif part == \"decoder\":\n",
    "            self.decoder = keras.models.load_model(path)\n",
    "        elif part == \"autoencoder\":\n",
    "            self.autoencoder = keras.models.load_model(path)\n",
    "        if x_train is not None:\n",
    "            self.fit_scaler(X_train)\n",
    "    \n",
    "\n",
    "    def _predict(self, df, model):\n",
    "        batches = self.scaler.transform(df)\n",
    "        batches = tuple(self._predictBatch(batch.reshape((1, *batch.shape)), model) for batch in batches)\n",
    "        batches = self._concatBatches(batches) \n",
    "        return self.scaler.reverse_transform(batches)\n",
    "    \n",
    "    def _predictBatch(self, batch, model):\n",
    "        return model.predict(batch)\n",
    "    \n",
    "    def _getBatches(self, arr, batch_size, axis=0):\n",
    "        n_batches = arr.shape[axis] // batch_size\n",
    "        result = np.array_split(arr, n_batches, axis=axis)\n",
    "        i = 0\n",
    "        while result[i].shape[0] != batch_size:\n",
    "            i += 1\n",
    "        result = result[i:]\n",
    "        return result\n",
    "    \n",
    "    def _concatBatches(self, batches, axis=0):\n",
    "        return np.concatenate(batches, axis=axis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AU_Stage_1(AutoEncoder):\n",
    "        def __init__(self, \n",
    "                 input_dim = (5, 58), \n",
    "                 encoded_dim = (1, 58), \n",
    "                 loss=\"mse\", \n",
    "                 optimizer=\"adadelta\", \n",
    "                 activation=(\"elu\", \"sigmoid\"),\n",
    "                 kernel = 5):\n",
    "        \n",
    "            self.input_dim = input_dim\n",
    "            self.encoded_dim = encoded_dim\n",
    "\n",
    "            #Encoder\n",
    "            self._inputs = Input(shape=input_dim)\n",
    "            self._flat = Flatten()(self._inputs)\n",
    "            self._dense = Dense(units=np.prod(encoded_dim), activation=activation[0])(self._flat)\n",
    "            self._encoded = Reshape(encoded_dim)(self._dense)\n",
    "\n",
    "            #Decoder\n",
    "            self._encoded_inputs = Input(shape=(encoded_dim[0]*encoded_dim[1],))\n",
    "            self._flat_decoded = Dense(input_dim[0]*input_dim[1], activation=activation[1])(self._encoded_inputs)\n",
    "            self._decoded = Reshape(input_dim)(self._flat_decoded)\n",
    "\n",
    "            #Models\n",
    "            self.encoder = Model(self._inputs, self._encoded)\n",
    "            self.decoder = Model(self._encoded_inputs, self._decoded)\n",
    "            self.autoencoder = Model(self._inputs, self.decoder(self.encoder(self._inputs)))\n",
    "            \n",
    "            self.autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "            class MinMaxScaler():\n",
    "\n",
    "                def __init__(self, minimum=None, maximum=None):\n",
    "                    self.minimum = minimum\n",
    "                    self.maximum = maximum\n",
    "\n",
    "                def fit_transform(self, X):\n",
    "                    if self.minimum is None or self.maximum is None:\n",
    "                        self.minimum = np.min(X, axis=(0, 1))\n",
    "                        self.maximum = np.max(X, axis=(0, 1))\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def transform(self, X):\n",
    "                    return (np.array(X) - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def reverse_transform(self, X_scl):\n",
    "                    return X_scl * (self.maximum - self.minimum) + self.minimum\n",
    "\n",
    "            self.scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AU_Stage_2(AutoEncoder):\n",
    "        def __init__(self, \n",
    "                 input_dim = (5, 58), \n",
    "                 encoded_dim = (1, 58), \n",
    "                 loss=\"mse\", \n",
    "                 optimizer=\"adadelta\", \n",
    "                 activation=(\"elu\", \"sigmoid\"),\n",
    "                 kernel = 5):\n",
    "        \n",
    "            self.input_dim = input_dim\n",
    "            self.encoded_dim = encoded_dim\n",
    "\n",
    "            #Encoder\n",
    "            self._inputs = Input(shape=input_dim)\n",
    "            self._conv = Conv1D(filters=encoded_dim[1], kernel_size=kernel)(self._inputs) \n",
    "            self._dense = Dense(units=np.prod(encoded_dim), activation=activation[0])(self._conv)\n",
    "            self._encoded = Reshape(encoded_dim)(self._dense)\n",
    "\n",
    "            #Decoder\n",
    "            self._encoded_inputs = Input(shape=encoded_dim)\n",
    "            self._flat_decoded = Dense(units=np.prod(input_dim), activation=activation[1])(self._encoded_inputs)\n",
    "            self._decoded = Reshape(input_dim)(self._flat_decoded)\n",
    "\n",
    "            #Models\n",
    "            self.encoder = Model(self._inputs, self._encoded)\n",
    "            self.decoder = Model(self._encoded_inputs, self._decoded)\n",
    "            self.autoencoder = Model(self._inputs, self.decoder(self.encoder(self._inputs)))\n",
    "\n",
    "            self.autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "            \n",
    "            class MinMaxScaler():\n",
    "\n",
    "                def __init__(self, minimum=None, maximum=None):\n",
    "                    self.minimum = minimum\n",
    "                    self.maximum = maximum\n",
    "\n",
    "                def fit_transform(self, X):\n",
    "                    if self.minimum is None or self.maximum is None:\n",
    "                        self.minimum = np.min(X, axis=(0, 1))\n",
    "                        self.maximum = np.max(X, axis=(0, 1))\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def transform(self, X):\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def reverse_transform(self, X_scl):\n",
    "                    return X_scl * (self.maximum - self.minimum) + self.minimum\n",
    "\n",
    "            self.scaler = MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AU_Stage_3(AutoEncoder):\n",
    "        def __init__(self, \n",
    "                 input_dim = (5, 58), \n",
    "                 encoded_dim = (1, 58), \n",
    "                 loss=\"mse\", \n",
    "                 optimizer=\"adadelta\", \n",
    "                 activation=(\"elu\", \"sigmoid\"),\n",
    "                 kernel = 5,\n",
    "                 folds=100):\n",
    "        \n",
    "            self.input_dim = input_dim\n",
    "            self.encoded_dim = encoded_dim\n",
    "\n",
    "            #Encoder\n",
    "            self._inputs = Input(shape=input_dim)\n",
    "            self._lambda = Lambda(lambda x: K.round(x * folds) / folds)(self._inputs)\n",
    "            self._lstm = LSTM(input_dim[1], return_sequences=True, dropout=0, recurrent_dropout=0.1)(self._lambda)\n",
    "            self._conv = Conv1D(filters=encoded_dim[1], kernel_size=input_dim[0])(self._lstm)\n",
    "            self._dense = Dense(units=np.prod(encoded_dim), activation=activation[0])(self._conv)\n",
    "            self._encoded = Reshape(encoded_dim)(self._dense)\n",
    "            \n",
    "            #Decoder\n",
    "            self._encoded_inputs = Input(shape=encoded_dim)\n",
    "            self._flat_decoded_1 = Dense(units=np.prod(input_dim), activation=activation[1])(self._encoded_inputs)\n",
    "            self._flat_decoded_2 = UpSampling1D(size=input_dim[0] // encoded_dim[0])(self._flat_decoded_1)\n",
    "            self._lstm_2 = LSTM(input_dim[1], return_sequences=True)(self._flat_decoded_2)\n",
    "            self._decoded = Reshape(input_dim)(self._lstm_2)\n",
    "            \n",
    "            #Models\n",
    "            self.encoder = Model(self._inputs, self._encoded)\n",
    "            self.decoder = Model(self._encoded_inputs, self._decoded)\n",
    "            self.autoencoder = Model(self._inputs, self.decoder(self.encoder(self._inputs)))\n",
    "            \n",
    "            self.autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "            \n",
    "            class MinMaxScaler():\n",
    "\n",
    "                def __init__(self, minimum=None, maximum=None):\n",
    "                    self.minimum = minimum\n",
    "                    self.maximum = maximum\n",
    "\n",
    "                def fit_transform(self, X):\n",
    "                    if self.minimum is None or self.maximum is None:\n",
    "                        self.minimum = np.min(X, axis=(0, 1))\n",
    "                        self.maximum = np.max(X, axis=(0, 1))  + 1e-10\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def transform(self, X):\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def reverse_transform(self, X_scl):\n",
    "                    return X_scl * (self.maximum - self.minimum) + self.minimum\n",
    "\n",
    "            self.scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AU_Stage_3_5(AutoEncoder):\n",
    "        def __init__(self, \n",
    "                 input_dim = (5, 58), \n",
    "                 encoded_dim = (1, 58), \n",
    "                 loss=\"mse\", \n",
    "                 optimizer=\"adadelta\", \n",
    "                 activation=(\"elu\", \"sigmoid\"),\n",
    "                 kernel = 5):\n",
    "        \n",
    "            self.input_dim = input_dim\n",
    "            self.encoded_dim = encoded_dim\n",
    "\n",
    "            #Encoder\n",
    "            self._inputs = Input(shape=input_dim)\n",
    "            self._lstm = GRU(input_dim[1], return_sequences=True, dropout=0, recurrent_dropout=0.1)(self._inputs)\n",
    "            self._conv = Conv1D(filters=encoded_dim[1], kernel_size=input_dim[0])(self._lstm)\n",
    "            self._dense = Dense(units=np.prod(encoded_dim), activation=activation[0])(self._conv)\n",
    "            self._encoded = Reshape(encoded_dim)(self._dense)\n",
    "\n",
    "            #Decoder\n",
    "            self._encoded_inputs = Input(shape=encoded_dim)\n",
    "            self._flat_decoded_1 = Dense(units=np.prod(input_dim), activation=activation[1])(self._encoded_inputs)\n",
    "            self._flat_decoded_2 = UpSampling1D(size=input_dim[0] // encoded_dim[0])(self._flat_decoded_1)\n",
    "            self._lstm_2 = GRU(input_dim[1], return_sequences=True)(self._flat_decoded_2)\n",
    "            self._decoded = Reshape(input_dim)(self._lstm_2)\n",
    "\n",
    "            #Models\n",
    "            self.encoder = Model(self._inputs, self._encoded)\n",
    "            self.decoder = Model(self._encoded_inputs, self._decoded)\n",
    "            self.autoencoder = Model(self._inputs, self.decoder(self.encoder(self._inputs)))\n",
    "\n",
    "            self.autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "            class MinMaxScaler():\n",
    "\n",
    "                def __init__(self, minimum=None, maximum=None):\n",
    "                    self.minimum = minimum\n",
    "                    self.maximum = maximum\n",
    "\n",
    "                def fit_transform(self, X):\n",
    "                    if self.minimum is None or self.maximum is None:\n",
    "                        self.minimum = np.min(X, axis=(0, 1))\n",
    "                        self.maximum = np.max(X, axis=(0, 1)) + 1e-10\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def transform(self, X):\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def reverse_transform(self, X_scl):\n",
    "                    return X_scl * (self.maximum - self.minimum) + self.minimum\n",
    "\n",
    "            self.scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AU_Stage_4(AutoEncoder):\n",
    "        def __init__(self, \n",
    "                 input_dim = (5, 58), \n",
    "                 encoded_dim = (1, 58), \n",
    "                 loss=\"mse\", \n",
    "                 optimizer=\"adadelta\", \n",
    "                 activation=(\"elu\", \"sigmoid\"),\n",
    "                 kernel = 5,\n",
    "                 folds = 100):\n",
    "        \n",
    "            self.input_dim = input_dim\n",
    "            self.encoded_dim = encoded_dim\n",
    "\n",
    "            #Encoder\n",
    "            self._inputs = Input(shape=input_dim)\n",
    "            self._lambda = Lambda(lambda x: K.round(x * folds) / folds)(self._inputs)\n",
    "            self._lstm = LSTM(input_dim[1], return_sequences=True, dropout=0, recurrent_dropout=0.1)(self._lambda)\n",
    "            self._reshape = Reshape((-1, input_dim[1] * input_dim[0]))(self._lstm)\n",
    "            self._dense = Dense(units=np.prod(encoded_dim), activation=activation[0])(self._reshape)\n",
    "            self._encoded = Reshape(encoded_dim)(self._dense)\n",
    "\n",
    "            #Decoder\n",
    "            self._encoded_inputs = Input(shape=encoded_dim)\n",
    "            self._flat_decoded_1 = Dense(units=np.prod(input_dim), activation=activation[1])(self._encoded_inputs)\n",
    "            self._lstm_2 = LSTM(input_dim[1] * input_dim[0], return_sequences=True)(self._flat_decoded_1)\n",
    "            self._decoded = Reshape((-1, input_dim[1]))(self._lstm_2)\n",
    "\n",
    "            #Models\n",
    "            self.encoder = Model(self._inputs, self._encoded)\n",
    "            self.decoder = Model(self._encoded_inputs, self._decoded)\n",
    "            self.autoencoder = Model(self._inputs, self.decoder(self.encoder(self._inputs)))\n",
    "\n",
    "            self.autoencoder.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "            class MinMaxScaler():\n",
    "\n",
    "                def __init__(self, minimum=None, maximum=None):\n",
    "                    self.minimum = minimum\n",
    "                    self.maximum = maximum\n",
    "\n",
    "                def fit_transform(self, X):\n",
    "                    if self.minimum is None or self.maximum is None:\n",
    "                        self.minimum = np.min(X, axis=(0, 1))\n",
    "                        self.maximum = np.max(X, axis=(0, 1)) + 1e-10\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def transform(self, X):\n",
    "                    return (X - self.minimum) / (self.maximum - self.minimum)\n",
    "\n",
    "                def reverse_transform(self, X_scl):\n",
    "                    return X_scl * (self.maximum - self.minimum) + self.minimum\n",
    "\n",
    "            self.scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epoch_numb = 50\n",
    "limit = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1663700, 5, 58)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we get data for all experiments in future\n",
    "au_1 = AU_Stage_1()\n",
    "data_set = au_1.prepare_clear_data(clear_data_path, limit=limit)\n",
    "train_data, test_data = train_test_split(data_set, random_state=0, test_size=0.3)\n",
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 4.3283e-04\n",
      "Epoch 2/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 1.8347e-04\n",
      "Epoch 3/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 1.3144e-04\n",
      "Epoch 4/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 1.1293e-04\n",
      "Epoch 5/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 1.0336e-04\n",
      "Epoch 6/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 9.6867e-05\n",
      "Epoch 7/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 9.1089e-05\n",
      "Epoch 8/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 8.5847e-05\n",
      "Epoch 9/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 8.1284e-05\n",
      "Epoch 10/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 7.7352e-05\n",
      "Epoch 11/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 7.3885e-05\n",
      "Epoch 12/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 7.0736e-05\n",
      "Epoch 13/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 6.7828e-05\n",
      "Epoch 14/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 6.5139e-05\n",
      "Epoch 15/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 6.2668e-05\n",
      "Epoch 16/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 6.0419e-05\n",
      "Epoch 17/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 5.8394e-05\n",
      "Epoch 18/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 5.6582e-05\n",
      "Epoch 19/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 5.4970e-05\n",
      "Epoch 20/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 5.3540e-05\n",
      "Epoch 21/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 5.2269e-05\n",
      "Epoch 22/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 5.1132e-05\n",
      "Epoch 23/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 5.0108e-05\n",
      "Epoch 24/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.9176e-05\n",
      "Epoch 25/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.8314e-05\n",
      "Epoch 26/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.7511e-05\n",
      "Epoch 27/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.6751e-05\n",
      "Epoch 28/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.6025e-05\n",
      "Epoch 29/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.5327e-05\n",
      "Epoch 30/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.4651e-05\n",
      "Epoch 31/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.3992e-05\n",
      "Epoch 32/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.3348e-05\n",
      "Epoch 33/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.2717e-05\n",
      "Epoch 34/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.2098e-05\n",
      "Epoch 35/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.1491e-05\n",
      "Epoch 36/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.0895e-05\n",
      "Epoch 37/50\n",
      "1164590/1164590 [==============================] - 45s 39us/step - loss: 4.0311e-05\n",
      "Epoch 38/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 3.9737e-05\n",
      "Epoch 39/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 3.9177e-05\n",
      "Epoch 40/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 3.8625e-05\n",
      "Epoch 41/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 3.8086e-05\n",
      "Epoch 42/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 3.7558e-05\n",
      "Epoch 43/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 3.7043e-05\n",
      "Epoch 44/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 3.6538e-05\n",
      "Epoch 45/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 3.6047e-05\n",
      "Epoch 46/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 3.5566e-05\n",
      "Epoch 47/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 3.5098e-05\n",
      "Epoch 48/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 3.4643e-05\n",
      "Epoch 49/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 3.4203e-05\n",
      "Epoch 50/50\n",
      "1164590/1164590 [==============================] - 46s 39us/step - loss: 3.3776e-05\n"
     ]
    }
   ],
   "source": [
    "au_1.fit(train_data, epochs=epoch_numb)\n",
    "folder = \"./model_stage_1/\"\n",
    "au_1.save(folder + \"au\")\n",
    "au_1.save(folder + \"en\")\n",
    "au_1.save(folder + \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dense\n",
      "\n",
      "Clear data\n",
      "0.8629372410722346\n",
      "5.079840092974387e-06\n",
      "5.368622227515531e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr = au_1._predict(test_data, au_1.autoencoder)\n",
    "ds = np.concatenate(test_data)\n",
    "p = np.concatenate(pr)\n",
    "print('\\nDense')\n",
    "print('\\nClear data')\n",
    "print(r2_score(ds, p))\n",
    "print(mean_absolute_error(ds, p))\n",
    "print(mean_squared_error(ds, p))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del au_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1164590/1164590 [==============================] - 47s 41us/step - loss: 3.9776e-04\n",
      "Epoch 2/50\n",
      "1164590/1164590 [==============================] - 47s 41us/step - loss: 1.5750e-04\n",
      "Epoch 3/50\n",
      "1164590/1164590 [==============================] - 47s 41us/step - loss: 1.2377e-04\n",
      "Epoch 4/50\n",
      "1164590/1164590 [==============================] - 47s 41us/step - loss: 1.0546e-04\n",
      "Epoch 5/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 9.6167e-05\n",
      "Epoch 6/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 8.8723e-05\n",
      "Epoch 7/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 8.2866e-05\n",
      "Epoch 8/50\n",
      "1164590/1164590 [==============================] - 47s 41us/step - loss: 7.8204e-05\n",
      "Epoch 9/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 7.4097e-05\n",
      "Epoch 10/50\n",
      "1164590/1164590 [==============================] - 47s 41us/step - loss: 7.0298e-05\n",
      "Epoch 11/50\n",
      "1164590/1164590 [==============================] - 48s 42us/step - loss: 6.6760e-05\n",
      "Epoch 12/50\n",
      "1164590/1164590 [==============================] - 47s 41us/step - loss: 6.3502e-05\n",
      "Epoch 13/50\n",
      "1164590/1164590 [==============================] - 47s 41us/step - loss: 6.0555e-05\n",
      "Epoch 14/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 5.7961e-05\n",
      "Epoch 15/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 5.5730e-05\n",
      "Epoch 16/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 5.3858e-05\n",
      "Epoch 17/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 5.2294e-05\n",
      "Epoch 18/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 5.0968e-05\n",
      "Epoch 19/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 4.9821e-05\n",
      "Epoch 20/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 4.8801e-05\n",
      "Epoch 21/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 4.7861e-05\n",
      "Epoch 22/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 4.6980e-05\n",
      "Epoch 23/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 4.6131e-05\n",
      "Epoch 24/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 4.5306e-05\n",
      "Epoch 25/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 4.4490e-05\n",
      "Epoch 26/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 4.3689e-05\n",
      "Epoch 27/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 4.2896e-05\n",
      "Epoch 28/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 4.2116e-05\n",
      "Epoch 29/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 4.1353e-05\n",
      "Epoch 30/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 4.0610e-05\n",
      "Epoch 31/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 3.9885e-05\n",
      "Epoch 32/50\n",
      "1164590/1164590 [==============================] - 48s 42us/step - loss: 3.9194e-05\n",
      "Epoch 33/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 3.8517e-05\n",
      "Epoch 34/50\n",
      "1164590/1164590 [==============================] - 48s 42us/step - loss: 3.7879e-05\n",
      "Epoch 35/50\n",
      "1164590/1164590 [==============================] - 48s 42us/step - loss: 3.7264e-05\n",
      "Epoch 36/50\n",
      "1164590/1164590 [==============================] - 48s 42us/step - loss: 3.6675e-05\n",
      "Epoch 37/50\n",
      "1164590/1164590 [==============================] - 48s 42us/step - loss: 3.6122e-05\n",
      "Epoch 38/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 3.5597e-05\n",
      "Epoch 39/50\n",
      "1164590/1164590 [==============================] - 48s 42us/step - loss: 3.5108e-05\n",
      "Epoch 40/50\n",
      "1164590/1164590 [==============================] - 48s 42us/step - loss: 3.4650e-05\n",
      "Epoch 41/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 3.4210e-05\n",
      "Epoch 42/50\n",
      "1164590/1164590 [==============================] - 48s 42us/step - loss: 3.3785e-05\n",
      "Epoch 43/50\n",
      "1164590/1164590 [==============================] - 48s 42us/step - loss: 3.3392e-05\n",
      "Epoch 44/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 3.2994e-05\n",
      "Epoch 45/50\n",
      "1164590/1164590 [==============================] - 48s 42us/step - loss: 3.2622e-05\n",
      "Epoch 46/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 3.2271e-05\n",
      "Epoch 47/50\n",
      "1164590/1164590 [==============================] - 48s 42us/step - loss: 3.1890e-05\n",
      "Epoch 48/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 3.1557e-05\n",
      "Epoch 49/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 3.1182e-05\n",
      "Epoch 50/50\n",
      "1164590/1164590 [==============================] - 48s 41us/step - loss: 3.0861e-05\n"
     ]
    }
   ],
   "source": [
    "au_2 = AU_Stage_2()\n",
    "# here we assume that batches are the same and use data from previous model to compare models correctly\n",
    "# data_set = au_2.prepare_clear_data(clear_data_path, limit=limit)\n",
    "# train_data, test_data = train_test_split(data_set, random_state=0, test_size=0.3)\n",
    "\n",
    "au_2.fit(train_data, epochs=epoch_numb)\n",
    "folder = \"./model_stage_2/\"\n",
    "au_2.save(folder + \"au\")\n",
    "au_2.save(folder + \"en\")\n",
    "au_2.save(folder + \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dense+Conv\n",
      "\n",
      "Clear data\n",
      "0.8721954383720413\n",
      "5.014486137700528e-06\n",
      "6.668714340613226e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr = au_2._predict(test_data, au_2.autoencoder)\n",
    "ds = np.concatenate(test_data)\n",
    "p = np.concatenate(pr)\n",
    "print('\\nDense+Conv')\n",
    "print('\\nClear data')\n",
    "print(r2_score(ds, p))\n",
    "print(mean_absolute_error(ds, p))\n",
    "print(mean_squared_error(ds, p))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del au_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1164590/1164590 [==============================] - 214s 183us/step - loss: 5.5478e-04\n",
      "Epoch 2/50\n",
      "1164590/1164590 [==============================] - 213s 183us/step - loss: 2.3129e-04\n",
      "Epoch 3/50\n",
      "1164590/1164590 [==============================] - 216s 185us/step - loss: 1.5157e-04\n",
      "Epoch 4/50\n",
      "1164590/1164590 [==============================] - 216s 186us/step - loss: 1.3814e-04\n",
      "Epoch 5/50\n",
      "1164590/1164590 [==============================] - 216s 185us/step - loss: 1.3555e-04\n",
      "Epoch 6/50\n",
      "1164590/1164590 [==============================] - 215s 185us/step - loss: 1.3338e-04\n",
      "Epoch 7/50\n",
      "1164590/1164590 [==============================] - 215s 185us/step - loss: 1.3034e-04\n",
      "Epoch 8/50\n",
      "1164590/1164590 [==============================] - 216s 185us/step - loss: 1.2434e-04\n",
      "Epoch 9/50\n",
      "1164590/1164590 [==============================] - 216s 186us/step - loss: 1.1747e-04\n",
      "Epoch 10/50\n",
      "1164590/1164590 [==============================] - 216s 186us/step - loss: 1.1148e-04\n",
      "Epoch 11/50\n",
      "1164590/1164590 [==============================] - 219s 188us/step - loss: 1.0683e-04\n",
      "Epoch 12/50\n",
      "1164590/1164590 [==============================] - 215s 185us/step - loss: 1.0400e-04\n",
      "Epoch 13/50\n",
      "1164590/1164590 [==============================] - 214s 184us/step - loss: 1.0214e-04\n",
      "Epoch 14/50\n",
      "1164590/1164590 [==============================] - 214s 184us/step - loss: 1.0062e-04\n",
      "Epoch 15/50\n",
      "1164590/1164590 [==============================] - 216s 185us/step - loss: 9.8868e-05\n",
      "Epoch 16/50\n",
      "1164590/1164590 [==============================] - 215s 185us/step - loss: 9.6775e-05\n",
      "Epoch 17/50\n",
      "1164590/1164590 [==============================] - 215s 185us/step - loss: 9.4653e-05\n",
      "Epoch 18/50\n",
      "1164590/1164590 [==============================] - 216s 185us/step - loss: 9.2946e-05\n",
      "Epoch 19/50\n",
      "1164590/1164590 [==============================] - 218s 187us/step - loss: 9.1438e-05\n",
      "Epoch 20/50\n",
      "1164590/1164590 [==============================] - 216s 185us/step - loss: 8.9666e-05\n",
      "Epoch 21/50\n",
      "1164590/1164590 [==============================] - 216s 185us/step - loss: 8.7837e-05\n",
      "Epoch 22/50\n",
      "1164590/1164590 [==============================] - 217s 186us/step - loss: 8.6064e-05\n",
      "Epoch 23/50\n",
      "1164590/1164590 [==============================] - 219s 188us/step - loss: 8.4538e-05\n",
      "Epoch 24/50\n",
      "1164590/1164590 [==============================] - 218s 187us/step - loss: 8.3427e-05\n",
      "Epoch 25/50\n",
      "1164590/1164590 [==============================] - 219s 188us/step - loss: 8.2581e-05\n",
      "Epoch 26/50\n",
      "1164590/1164590 [==============================] - 219s 188us/step - loss: 8.1827e-05\n",
      "Epoch 27/50\n",
      "1164590/1164590 [==============================] - 219s 188us/step - loss: 8.1145e-05\n",
      "Epoch 28/50\n",
      "1164590/1164590 [==============================] - 219s 188us/step - loss: 8.0430e-05\n",
      "Epoch 29/50\n",
      "1164590/1164590 [==============================] - 219s 188us/step - loss: 7.9665e-05\n",
      "Epoch 30/50\n",
      "1164590/1164590 [==============================] - 219s 188us/step - loss: 7.8841e-05\n",
      "Epoch 31/50\n",
      "1164590/1164590 [==============================] - 220s 189us/step - loss: 7.7953e-05\n",
      "Epoch 32/50\n",
      "1164590/1164590 [==============================] - 221s 190us/step - loss: 7.7025e-05\n",
      "Epoch 33/50\n",
      "1164590/1164590 [==============================] - 221s 190us/step - loss: 7.6053e-05\n",
      "Epoch 34/50\n",
      "1164590/1164590 [==============================] - 220s 189us/step - loss: 7.5026e-05\n",
      "Epoch 35/50\n",
      "1164590/1164590 [==============================] - 221s 190us/step - loss: 7.4056e-05\n",
      "Epoch 36/50\n",
      "1164590/1164590 [==============================] - 221s 190us/step - loss: 7.3079e-05\n",
      "Epoch 37/50\n",
      "1164590/1164590 [==============================] - 221s 190us/step - loss: 7.2079e-05\n",
      "Epoch 38/50\n",
      "1164590/1164590 [==============================] - 222s 190us/step - loss: 7.1098e-05\n",
      "Epoch 39/50\n",
      "1164590/1164590 [==============================] - 222s 190us/step - loss: 7.0105e-05\n",
      "Epoch 40/50\n",
      "1164590/1164590 [==============================] - 221s 190us/step - loss: 6.9240e-05\n",
      "Epoch 41/50\n",
      "1164590/1164590 [==============================] - 221s 190us/step - loss: 6.8367e-05\n",
      "Epoch 42/50\n",
      "1164590/1164590 [==============================] - 222s 190us/step - loss: 6.7616e-05\n",
      "Epoch 43/50\n",
      "1164590/1164590 [==============================] - 222s 191us/step - loss: 6.6902e-05\n",
      "Epoch 44/50\n",
      "1164590/1164590 [==============================] - 216s 186us/step - loss: 6.6231e-05\n",
      "Epoch 45/50\n",
      "1164590/1164590 [==============================] - 215s 185us/step - loss: 6.5600e-05\n",
      "Epoch 46/50\n",
      "1164590/1164590 [==============================] - 215s 184us/step - loss: 6.5003e-05\n",
      "Epoch 47/50\n",
      "1164590/1164590 [==============================] - 214s 184us/step - loss: 6.4375e-05\n",
      "Epoch 48/50\n",
      "1164590/1164590 [==============================] - 215s 184us/step - loss: 6.3766e-05\n",
      "Epoch 49/50\n",
      "1164590/1164590 [==============================] - 216s 185us/step - loss: 6.3182e-05\n",
      "Epoch 50/50\n",
      "1164590/1164590 [==============================] - 216s 186us/step - loss: 6.2566e-05\n"
     ]
    }
   ],
   "source": [
    "au_3 = AU_Stage_3()\n",
    "# here we assume that batches are the same and use data from previous model to compare models correctly\n",
    "# data_set = au_3.prepare_clear_data(clear_data_path, limit=limit)\n",
    "# train_data, test_data = train_test_split(data_set, random_state=0, test_size=0.3)\n",
    "\n",
    "au_3.fit(train_data, epochs=epoch_numb)\n",
    "folder = \"./model_stage_3/\"\n",
    "au_3.save(folder + \"au\")\n",
    "au_3.save(folder + \"en\")\n",
    "au_3.save(folder + \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dense+Conv+LSTM\n",
      "\n",
      "Clear data\n",
      "0.7572858637508183\n",
      "9.641932782303063e-06\n",
      "1.3170053543019765e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr = au_3._predict(test_data, au_3.autoencoder)\n",
    "ds = np.concatenate(test_data)\n",
    "p = np.concatenate(pr)\n",
    "print(\"\\nDense+Conv+LSTM\")\n",
    "print('\\nClear data')\n",
    "print(r2_score(ds, p))\n",
    "print(mean_absolute_error(ds, p))\n",
    "print(mean_squared_error(ds, p))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del au_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1164590/1164590 [==============================] - 186s 160us/step - loss: 3.2270e-04\n",
      "Epoch 2/50\n",
      "1164590/1164590 [==============================] - 186s 160us/step - loss: 1.2192e-04\n",
      "Epoch 3/50\n",
      "1164590/1164590 [==============================] - 187s 160us/step - loss: 1.0239e-04\n",
      "Epoch 4/50\n",
      "1164590/1164590 [==============================] - 187s 160us/step - loss: 9.1380e-05\n",
      "Epoch 5/50\n",
      "1164590/1164590 [==============================] - 189s 163us/step - loss: 8.3100e-05\n",
      "Epoch 6/50\n",
      "1164590/1164590 [==============================] - 189s 162us/step - loss: 7.7032e-05\n",
      "Epoch 7/50\n",
      "1164590/1164590 [==============================] - 189s 163us/step - loss: 7.2240e-05\n",
      "Epoch 8/50\n",
      "1164590/1164590 [==============================] - 190s 163us/step - loss: 6.9036e-05\n",
      "Epoch 9/50\n",
      "1164590/1164590 [==============================] - 190s 163us/step - loss: 6.6678e-05\n",
      "Epoch 10/50\n",
      "1164590/1164590 [==============================] - 189s 163us/step - loss: 6.4504e-05\n",
      "Epoch 11/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 6.2598e-05\n",
      "Epoch 12/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 6.0468e-05\n",
      "Epoch 13/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 5.8449e-05\n",
      "Epoch 14/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 5.6676e-05\n",
      "Epoch 15/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 5.5253e-05\n",
      "Epoch 16/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 5.4026e-05\n",
      "Epoch 17/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 5.2970e-05\n",
      "Epoch 18/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 5.1949e-05\n",
      "Epoch 19/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 5.1123e-05\n",
      "Epoch 20/50\n",
      "1164590/1164590 [==============================] - 192s 164us/step - loss: 5.0169e-05\n",
      "Epoch 21/50\n",
      "1164590/1164590 [==============================] - 192s 165us/step - loss: 4.9551e-05\n",
      "Epoch 22/50\n",
      "1164590/1164590 [==============================] - 192s 165us/step - loss: 4.8776e-05\n",
      "Epoch 23/50\n",
      "1164590/1164590 [==============================] - 192s 165us/step - loss: 4.8062e-05\n",
      "Epoch 24/50\n",
      "1164590/1164590 [==============================] - 190s 164us/step - loss: 4.7354e-05\n",
      "Epoch 25/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 4.6842e-05\n",
      "Epoch 26/50\n",
      "1164590/1164590 [==============================] - 190s 163us/step - loss: 4.6205e-05\n",
      "Epoch 27/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 4.5531e-05\n",
      "Epoch 28/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 4.4894e-05\n",
      "Epoch 29/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 4.4370e-05\n",
      "Epoch 30/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 4.3734e-05\n",
      "Epoch 31/50\n",
      "1164590/1164590 [==============================] - 190s 163us/step - loss: 4.3207e-05\n",
      "Epoch 32/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 4.2574e-05\n",
      "Epoch 33/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 4.2069e-05\n",
      "Epoch 34/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 4.1546e-05\n",
      "Epoch 35/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 4.1122e-05\n",
      "Epoch 36/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 4.0629e-05\n",
      "Epoch 37/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 4.0219e-05\n",
      "Epoch 38/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 3.9836e-05\n",
      "Epoch 39/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 3.9405e-05\n",
      "Epoch 40/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 3.9122e-05\n",
      "Epoch 41/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 3.8848e-05\n",
      "Epoch 42/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 3.8445e-05\n",
      "Epoch 43/50\n",
      "1164590/1164590 [==============================] - 191s 164us/step - loss: 3.8225e-05\n",
      "Epoch 44/50\n",
      "1164590/1164590 [==============================] - 190s 163us/step - loss: 3.8012e-05\n",
      "Epoch 45/50\n",
      "1164590/1164590 [==============================] - 190s 163us/step - loss: 3.7771e-05\n",
      "Epoch 46/50\n",
      "1164590/1164590 [==============================] - 190s 163us/step - loss: 3.7543e-05\n",
      "Epoch 47/50\n",
      "1164590/1164590 [==============================] - 190s 164us/step - loss: 3.7284e-05\n",
      "Epoch 48/50\n",
      "1164590/1164590 [==============================] - 190s 163us/step - loss: 3.7096e-05\n",
      "Epoch 49/50\n",
      "1164590/1164590 [==============================] - 190s 163us/step - loss: 3.6872e-05\n",
      "Epoch 50/50\n",
      "1164590/1164590 [==============================] - 190s 163us/step - loss: 3.6668e-05\n"
     ]
    }
   ],
   "source": [
    "au_3_5 = AU_Stage_3_5()\n",
    "# here we assume that batches are the same and use data from previous model to compare models correctly\n",
    "# data_set = au_3_5.prepare_clear_data(clear_data_path, limit=limit)\n",
    "# train_data, test_data = train_test_split(data_set, random_state=0, test_size=0.3)\n",
    "\n",
    "au_3_5.fit(train_data, epochs=epoch_numb)\n",
    "folder = \"./model_stage_3_5/\"\n",
    "au_3_5.save(folder + \"au\")\n",
    "au_3_5.save(folder + \"en\")\n",
    "au_3_5.save(folder + \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dense+Conv+GRU\n",
      "\n",
      "Clear data\n",
      "0.8661866968120966\n",
      "5.41238914054588e-06\n",
      "1.3698147217852704e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr = au_3_5._predict(test_data, au_3_5.autoencoder)\n",
    "ds = np.concatenate(test_data)\n",
    "p = np.concatenate(pr)\n",
    "print(\"\\nDense+Conv+GRU\")\n",
    "print('\\nClear data')\n",
    "print(r2_score(ds, p))\n",
    "print(mean_absolute_error(ds, p))\n",
    "print(mean_squared_error(ds, p))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del au_3_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1164590/1164590 [==============================] - 384s 329us/step - loss: 6.6335e-04\n",
      "Epoch 2/50\n",
      "1164590/1164590 [==============================] - 383s 329us/step - loss: 2.8707e-04\n",
      "Epoch 3/50\n",
      "1164590/1164590 [==============================] - 384s 330us/step - loss: 2.6696e-04\n",
      "Epoch 4/50\n",
      "1164590/1164590 [==============================] - 384s 330us/step - loss: 2.3863e-04\n",
      "Epoch 5/50\n",
      "1164590/1164590 [==============================] - 384s 330us/step - loss: 1.6729e-04\n",
      "Epoch 6/50\n",
      "1164590/1164590 [==============================] - 384s 330us/step - loss: 1.5943e-04\n",
      "Epoch 7/50\n",
      "1164590/1164590 [==============================] - 384s 330us/step - loss: 1.5489e-04\n",
      "Epoch 8/50\n",
      "1164590/1164590 [==============================] - 385s 330us/step - loss: 1.4149e-04\n",
      "Epoch 9/50\n",
      "1164590/1164590 [==============================] - 384s 330us/step - loss: 1.1903e-04\n",
      "Epoch 10/50\n",
      "1164590/1164590 [==============================] - 384s 330us/step - loss: 1.1265e-04\n",
      "Epoch 11/50\n",
      "1164590/1164590 [==============================] - 384s 330us/step - loss: 1.1055e-04\n",
      "Epoch 12/50\n",
      "1164590/1164590 [==============================] - 385s 330us/step - loss: 1.0882e-04\n",
      "Epoch 13/50\n",
      "1164590/1164590 [==============================] - 385s 330us/step - loss: 1.0646e-04\n",
      "Epoch 14/50\n",
      "1164590/1164590 [==============================] - 384s 330us/step - loss: 1.0214e-04\n",
      "Epoch 15/50\n",
      "1164590/1164590 [==============================] - 384s 329us/step - loss: 9.6384e-05\n",
      "Epoch 16/50\n",
      "1164590/1164590 [==============================] - 384s 330us/step - loss: 9.3109e-05\n",
      "Epoch 17/50\n",
      "1164590/1164590 [==============================] - 399s 342us/step - loss: 9.1438e-05\n",
      "Epoch 18/50\n",
      "1164590/1164590 [==============================] - 413s 355us/step - loss: 9.0321e-05\n",
      "Epoch 19/50\n",
      "1164590/1164590 [==============================] - 520s 447us/step - loss: 8.9445e-05\n",
      "Epoch 20/50\n",
      "1164590/1164590 [==============================] - 522s 448us/step - loss: 8.8645e-05\n",
      "Epoch 21/50\n",
      "1164590/1164590 [==============================] - 522s 448us/step - loss: 8.7796e-05\n",
      "Epoch 22/50\n",
      "1164590/1164590 [==============================] - 521s 447us/step - loss: 8.6892e-05\n",
      "Epoch 23/50\n",
      "1164590/1164590 [==============================] - 521s 448us/step - loss: 8.5638e-05\n",
      "Epoch 24/50\n",
      "1164590/1164590 [==============================] - 521s 448us/step - loss: 8.4223e-05\n",
      "Epoch 25/50\n",
      "1164590/1164590 [==============================] - 521s 447us/step - loss: 8.2535e-05\n",
      "Epoch 26/50\n",
      "1164590/1164590 [==============================] - 521s 448us/step - loss: 8.0776e-05\n",
      "Epoch 27/50\n",
      "1164590/1164590 [==============================] - 522s 448us/step - loss: 7.8892e-05\n",
      "Epoch 28/50\n",
      "1164590/1164590 [==============================] - 523s 449us/step - loss: 7.6995e-05\n",
      "Epoch 29/50\n",
      "1164590/1164590 [==============================] - 522s 448us/step - loss: 7.5533e-05\n",
      "Epoch 30/50\n",
      "1164590/1164590 [==============================] - 523s 449us/step - loss: 7.4455e-05\n",
      "Epoch 31/50\n",
      "1164590/1164590 [==============================] - 523s 449us/step - loss: 7.3632e-05\n",
      "Epoch 32/50\n",
      "1164590/1164590 [==============================] - 525s 451us/step - loss: 7.2938e-05\n",
      "Epoch 33/50\n",
      "1164590/1164590 [==============================] - 527s 453us/step - loss: 7.2325e-05\n",
      "Epoch 34/50\n",
      "1164590/1164590 [==============================] - 526s 452us/step - loss: 7.1746e-05\n",
      "Epoch 35/50\n",
      "1164590/1164590 [==============================] - 527s 452us/step - loss: 7.1176e-05\n",
      "Epoch 36/50\n",
      "1164590/1164590 [==============================] - 526s 452us/step - loss: 7.0524e-05\n",
      "Epoch 37/50\n",
      "1164590/1164590 [==============================] - 527s 452us/step - loss: 6.9818e-05\n",
      "Epoch 38/50\n",
      "1164590/1164590 [==============================] - 527s 452us/step - loss: 6.8916e-05\n",
      "Epoch 39/50\n",
      "1164590/1164590 [==============================] - 527s 453us/step - loss: 6.7827e-05\n",
      "Epoch 40/50\n",
      "1164590/1164590 [==============================] - 527s 453us/step - loss: 6.6572e-05\n",
      "Epoch 41/50\n",
      "1164590/1164590 [==============================] - 526s 452us/step - loss: 6.5337e-05\n",
      "Epoch 42/50\n",
      "1164590/1164590 [==============================] - 526s 452us/step - loss: 6.4312e-05\n",
      "Epoch 43/50\n",
      "1164590/1164590 [==============================] - 527s 452us/step - loss: 6.3517e-05\n",
      "Epoch 44/50\n",
      "1164590/1164590 [==============================] - 528s 453us/step - loss: 6.2889e-05\n",
      "Epoch 45/50\n",
      "1164590/1164590 [==============================] - 527s 453us/step - loss: 6.2321e-05\n",
      "Epoch 46/50\n",
      "1164590/1164590 [==============================] - 527s 453us/step - loss: 6.1809e-05\n",
      "Epoch 47/50\n",
      "1164590/1164590 [==============================] - 527s 453us/step - loss: 6.1321e-05\n",
      "Epoch 48/50\n",
      "1164590/1164590 [==============================] - 528s 453us/step - loss: 6.0826e-05\n",
      "Epoch 49/50\n",
      "1164590/1164590 [==============================] - 528s 453us/step - loss: 6.0325e-05\n",
      "Epoch 50/50\n",
      "1164590/1164590 [==============================] - 528s 453us/step - loss: 5.9778e-05\n"
     ]
    }
   ],
   "source": [
    "au_4 = AU_Stage_4()\n",
    "# here we assume that batches are the same and use data from previous model to compare models correctly\n",
    "# data_set = au_4.prepare_clear_data(clear_data_path, limit=limit)\n",
    "# train_data, test_data = train_test_split(data_set, random_state=0, test_size=0.3)\n",
    "\n",
    "au_4.fit(train_data, epochs=epoch_numb)\n",
    "folder = \"./model_stage_4/\"\n",
    "au_4.save(folder + \"au\")\n",
    "au_4.save(folder + \"en\")\n",
    "au_4.save(folder + \"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dense+LSTM\n",
      "\n",
      "Clear data\n",
      "0.7932128982892078\n",
      "4.4900777431201955e-06\n",
      "1.0678865418996525e-08\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pr = au_4._predict(test_data, au_4.autoencoder)\n",
    "ds = np.concatenate(test_data)\n",
    "p = np.concatenate(pr)\n",
    "print(\"\\nDense+LSTM\")\n",
    "print('\\nClear data')\n",
    "print(r2_score(ds, p))\n",
    "print(mean_absolute_error(ds, p))\n",
    "print(mean_squared_error(ds, p))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del au_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
